{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d015de-a82f-4769-81e3-8b4803e210a5",
     "showTitle": true,
     "title": "IMPORTS"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import to_json,from_json,col,explode,lit,to_timestamp,input_file_name,current_timestamp\n",
    "from pyspark.sql import DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9899de93-3146-4657-897f-51db8421dbb7",
     "showTitle": true,
     "title": "Load flights_bronze"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name = 'flights_bronze'\n",
    ")\n",
    "def get_flights_data():\n",
    "    df = spark.readStream.format(\"cloudFiles\")\\\n",
    "        .option(\"cloudFiles.format\",\"parquet\")\\\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\\\n",
    "        .option(\"recursiveFileLookup\", \"true\")\\\n",
    "        .option(\"cloudFiles.schemaLocation\",\"dbfs:/mnt/zathura/checkpoints/raw/tables/flights/\")\\\n",
    "        .load(\"dbfs:/mnt/data/raw/Postgress/flights/\")\n",
    "\n",
    "    df = df.withColumn(\"filename\",input_file_name()).withColumn(\"fileTimestamp\",current_timestamp())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "366a60eb-cf65-4a4b-a407-f483e4f343b1",
     "showTitle": true,
     "title": "Load payments_bronze"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name = 'payments_bronze'\n",
    ")\n",
    "def get_payments_data():\n",
    "    df = spark.readStream\\\n",
    "        .format(\"cloudFiles\")\\\n",
    "        .option(\"cloudFiles.format\",\"parquet\")\\\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\\\n",
    "        .option(\"recursiveFileLookup\", \"true\")\\\n",
    "        .option(\"cloudFiles.schemaLocation\",\"dbfs:/mnt/zathura/checkpoints/raw/tables/payments/\")\\\n",
    "        .load(\"dbfs:/mnt/data/raw/Postgress/payments/\")\n",
    "\n",
    "    df = df.withColumn(\"filename\",input_file_name()).withColumn(\"fileTimestamp\",current_timestamp())    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca3907ff-5d46-45e3-ae85-b76914eed030",
     "showTitle": true,
     "title": "Load silver_flight_schedules"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name = 'silver_flight_schedules'\n",
    ")\n",
    "def get_flights_data():\n",
    "    df = spark.readStream.format(\"cloudFiles\")\\\n",
    "        .option(\"cloudFiles.format\",\"parquet\")\\\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\\\n",
    "        .option(\"recursiveFileLookup\", \"true\")\\\n",
    "        .option(\"cloudFiles.schemaLocation\",\"dbfs:/mnt/zathura/checkpoints/raw/tables/flight_schedules/\")\\\n",
    "        .load(\"dbfs:/mnt/data/raw/Postgress/flight_schedules/\")\n",
    "\n",
    "    df = df.withColumn(\"filename\",input_file_name()).withColumn(\"fileTimestamp\",current_timestamp())\n",
    "\n",
    "    df = df.select(\"name\",\"frequency\",\"source\",\"destination\",\"timings\",\"filename\",\"fileTimestamp\")\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "584fb628-c34b-4efd-b7f3-b2fcaa6fe80f",
     "showTitle": true,
     "title": "Load silver_airplanes"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name = 'silver_airplanes'\n",
    ")\n",
    "def get_flights_data():\n",
    "    df = spark.readStream.format(\"cloudFiles\")\\\n",
    "        .option(\"cloudFiles.format\",\"parquet\")\\\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\\\n",
    "        .option(\"recursiveFileLookup\", \"true\")\\\n",
    "        .option(\"cloudFiles.schemaLocation\",\"dbfs:/mnt/zathura/checkpoints/raw/tables/airplanes/\")\\\n",
    "        .load(\"dbfs:/mnt/data/raw/Postgress/airplanes/\")\n",
    "\n",
    "    df = df.withColumn(\"filename\",input_file_name()).withColumn(\"fileTimestamp\",current_timestamp())\n",
    "\n",
    "    df = df.select(\"airplane_id\",\"name\",\"type\",\"total_seats\",\"fuel_capacity\",\"range\",\"filename\",\"fileTimestamp\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95af038d-2770-4d5d-953b-1cbc8a731960",
     "showTitle": true,
     "title": "Load silver_flights"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name = 'silver_flights',\n",
    "    comment=\"The cleaned flight data to select specified columns and partitioned by flight_date\",\n",
    "    partition_cols=[\"flight_date\"],\n",
    "    table_properties={\n",
    "        \"myCompanyPipeline.quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect(\"valid ID\", \"id IS NOT NULL\")\n",
    "def enriching_flights():\n",
    "    df = dlt.readStream(\"flights_bronze\")\n",
    "    df = df.withColumn(\"flight_date\",df.departure_time.cast(\"DATE\"))\n",
    "    df = df.withColumn(\"departure_timestamp\",to_timestamp(col(\"departure_time\")))\n",
    "    df = df.withColumn(\"arrival_timestamp\",to_timestamp(col(\"arrival_time\")))\n",
    "    df = df.select(\"id\",\"flight_code\",\"source_airport\",\"destination_airport\",\"source_city\",\"destination_city\",\"estimated_departure_time\",\"departure_timestamp\",\"estimated_arrival_time\",\"arrival_timestamp\",\"total_passengers\",\"status\",\"filename\",\"fileTimestamp\",\"flight_date\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e2d1ba5-9e09-4a3a-bc42-1ad7e14d47ca",
     "showTitle": true,
     "title": "Load silver_payments"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(\n",
    "    name = 'silver_payments',\n",
    "    comment=\"The cleaned flight - payment data to select specified columns and partitioned by payment_date\",\n",
    "    partition_cols=[\"payment_date\"],\n",
    "    table_properties={\n",
    "        \"myCompanyPipeline.quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_all({\"valid TransactionID\":\"transaction_id IS NOT NULL\", \"valid amount\" : \"amount IS NOT NULL  AND amount > 0\"})\n",
    "def enriching_payments():\n",
    "    df = dlt.readStream(\"payments_bronze\")\n",
    "    df = df.withColumn(\"payment_date\",df.time_of_payment.cast(\"DATE\"))\n",
    "    df = df.select(\"flight_id\",\"first_name\",\"last_name\",\"contact_number\",\"email_id\",\"seat_number\",\"mode_of_payment\",\"time_of_payment\",\"amount\",\"transaction_id\",\"agent\",\"filename\",\"fileTimestamp\",\"payment_date\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feba1f47-1f5f-4a92-80d7-c5f43a6b52de",
     "showTitle": true,
     "title": "Load silver_passengers"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@dlt.table(\n",
    "    name = 'silver_passengers',\n",
    "    comment=\"The cleaned flight data to select specified columns and partitioned by order_date\",\n",
    "    table_properties={\n",
    "        \"myCompanyPipeline.quality\": \"silver\",\n",
    "        \"pipelines.autoOptimize.managed\": \"true\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_all({\"valid bookingID\" : \"bookingID IS NOT NULL\", \"valid seat with person\" : \"seat IS NOT NULL AND first_name IS NOT NULL\"})\n",
    "def getPassenger_fromflights():\n",
    "    \n",
    "    # Define the schema\n",
    "    schema = ArrayType(StructType([\n",
    "        StructField(\"first_name\", StringType()),\n",
    "        StructField(\"last_name\", StringType()),\n",
    "        StructField(\"age\", IntegerType()),\n",
    "        StructField(\"gender\", StringType()),\n",
    "        StructField(\"contact_number\", StringType()),\n",
    "        StructField(\"email_id\", StringType()),\n",
    "        StructField(\"aadhar_number\", StringType()),\n",
    "        StructField(\"passport_number\", StringType()),\n",
    "        StructField(\"seating_class\", StringType()),\n",
    "        StructField(\"seat\", StringType()),\n",
    "        StructField(\"bookingID\", StringType())\n",
    "    ]))\n",
    "\n",
    "\n",
    "\n",
    "    df = dlt.readStream(\"flights_bronze\").select(\"id\",\"passenger_data\",\"filename\",\"fileTimestamp\")\n",
    "    df = df.withColumn(\"json_passenger_data\", from_json(\"passenger_data\", schema))\n",
    "    df = df.withColumn(\"exploded_passenger_data\", explode(\"json_passenger_data\"))\n",
    "    df = df.select(\"id\",\"filename\",\"fileTimestamp\",\"exploded_passenger_data.*\")\n",
    "    df = df.select(\"id\",\"first_name\",\"last_name\",\"age\",\"gender\",\"contact_number\",\"email_id\",\"aadhar_number\",\"passport_number\",\"seating_class\",\"seat\",\"bookingID\",\"filename\",\"fileTimestamp\")\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
